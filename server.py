"""
Noah Hobson, nhobs1999@gmail.com
server.py, part of the autocomplete program
Platform: agnostic

This program handles multiple aspects of the backend of the SDP search autocomplete functionality.
1) It creates a simple http server which listens to GET
    requests and responds with a list of similar queries in JSON format.
2) It creates a multiprocessing process pool based off of the number of
    available CPU cores (logical cores, aka threads)
3) It reads in the CSV file generated by parmenides (see settings.py)
    and splits it into chunks, one for each process.

Each dataset is roughly the same size, based on the number of CPU threads available.
Example: My CPU has 6 cores, 12 threads, so 12 splits are made.
My test CSV file has 13052 entries. This means about 1088 entries per dataset.
Technically speaking, this isn't a lot, but when trying to calculate search terms in real time, it adds up.

TODO: replace http server with https server (need certificate of authority first)
"""

import http.server
import multiprocessing as mp
import os
import signal
import socketserver
import sys
import urllib.parse as parse

import pandas

import autocomplete

# In case the better regex library is not available, use the standard library regex
try:
    import regex as re
except:
    import re

# We only want to run the majority of this code once
# Multiprocessing will run anything not in main as many times as it creates processes
# So this is the basic protection against that issue
# (mostly on Windows, though affects Linux too)
if __name__ == "__main__":
    PORT = 8000
    NUM_THREADS = max(os.cpu_count(), mp.cpu_count())

    # Read in CSV file from parmenides
    print("Reading file...")
    results = pandas.read_csv("parmenides_results.csv")
    len_results = len(results)
    print("File read. Results:", len_results)

    splits = []  # The smaller datasets used by each process

    # Each dataset is roughly the same size.
    for i in range(NUM_THREADS):
        start = int(i * len_results / NUM_THREADS)
        end = int((i + 1) * len_results / NUM_THREADS)
        splits.append(results[start:end])

    print("Created datasets for process pool")

    # Creates the process pool with as many processes as logical processors available
    with mp.Pool(NUM_THREADS) as p:
        print("Created process pool with", NUM_THREADS, "processes")

        # Override the GET protocol with our own which generates search terms
        # (see autocomplete.py)
        class AutocompleteServer(http.server.BaseHTTPRequestHandler):
            def do_GET(self):
                self.send_response(200)
                self.send_header("Content-type", "text/json")
                self.send_header("Access-Control-Allow-Origin", "*")
                self.end_headers()
                search_terms = parse.unquote(self.path[1:]).lower().strip()
                search_terms = re.sub(r'[^a-z]', ' ', search_terms)
                self.wfile.write(
                    bytes(autocomplete.search_json(search_terms, splits, p), "utf-8")
                )

        # Override the signal for SIGINT (ctrl+c on command line) to gracefully shut down server
        def signal_handler(sig, frame):
            global server
            print("Received ctrl+c, exiting...")
            server.server_close()
            sys.exit(0)

        signal.signal(signal.SIGINT, signal_handler)

        # Create the server using our inherited AutocompleteServer class
        # Spin-wait on the server to guarantee it eventually gets served
        print("Creating server...")
        server = None
        while server == None:
            try:
                server = socketserver.ThreadingTCPServer(("", PORT), AutocompleteServer)
            except:
                pass
        
        # These help the server shut down more gracefully when used with multiprocessing libraries
        server.allow_reuse_address = True
        server.daemon_threads = True

        try:
            while True:
                print("Serving on port", PORT)
                sys.stdout.flush()
                server.serve_forever()
        except KeyboardInterrupt: # On Windows, sometimes the signal handler doesn't work
            signal_handler(None, None)
